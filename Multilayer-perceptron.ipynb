{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VqrAWH7CkflLLjnQwu5-ORgvfz9FpUVb","timestamp":1739170171941},{"file_id":"16fdqKEaqQCXEpVEeQ-fyS0mrUoKhz5Ho","timestamp":1739170105973}],"authorship_tag":"ABX9TyOrjw8LVdN3rQRb6PH9xlC+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFjx9sbce12B","executionInfo":{"status":"ok","timestamp":1739168020317,"user_tz":-330,"elapsed":3395,"user":{"displayName":"Shreya Charlie","userId":"14569113716530734218"}},"outputId":"58c3131f-047f-43aa-ae5f-718228107f75"},"outputs":[{"output_type":"stream","name":"stdout","text":["   protocol address  type of connection  number of packets  \\\n","0              1234                   1                 45   \n","1              4563                   1                 67   \n","2              1256                   0                 23   \n","3              8967                   1                 45   \n","4              2315                   0                 78   \n","\n","   data transfer rate purchased  \n","0               45000       Yes  \n","1               56000       Yes  \n","2               30000       Yes  \n","3               47000       Yes  \n","4               62000       Yes  \n","Iteration 1, loss = 0.68335293\n","Iteration 2, loss = 0.68260395\n","Iteration 3, loss = 0.68186423\n","Iteration 4, loss = 0.68113415\n","Iteration 5, loss = 0.68041409\n","Iteration 6, loss = 0.67970442\n","Iteration 7, loss = 0.67900551\n","Iteration 8, loss = 0.67831774\n","Iteration 9, loss = 0.67764146\n","Iteration 10, loss = 0.67697705\n","Iteration 11, loss = 0.67632484\n","Iteration 12, loss = 0.67568519\n","Iteration 13, loss = 0.67505843\n","Iteration 14, loss = 0.67444488\n","Iteration 15, loss = 0.67384486\n","Iteration 16, loss = 0.67325867\n","Iteration 17, loss = 0.67268660\n","Iteration 18, loss = 0.67212891\n","Iteration 19, loss = 0.67158586\n","Iteration 20, loss = 0.67105769\n","Iteration 21, loss = 0.67054461\n","Iteration 22, loss = 0.67004682\n","Iteration 23, loss = 0.66956448\n","Iteration 24, loss = 0.66909776\n","Iteration 25, loss = 0.66864677\n","Iteration 26, loss = 0.66821162\n","Iteration 27, loss = 0.66779238\n","Iteration 28, loss = 0.66738909\n","Iteration 29, loss = 0.66700178\n","Iteration 30, loss = 0.66663043\n","Iteration 31, loss = 0.66627500\n","Iteration 32, loss = 0.66593543\n","Iteration 33, loss = 0.66561161\n","Iteration 34, loss = 0.66530340\n","Iteration 35, loss = 0.66501065\n","Iteration 36, loss = 0.66473316\n","Iteration 37, loss = 0.66447071\n","Iteration 38, loss = 0.66422305\n","Iteration 39, loss = 0.66398989\n","Iteration 40, loss = 0.66377091\n","Iteration 41, loss = 0.66356579\n","Iteration 42, loss = 0.66337414\n","Iteration 43, loss = 0.66319559\n","Iteration 44, loss = 0.66302972\n","Iteration 45, loss = 0.66287610\n","Iteration 46, loss = 0.66273426\n","Iteration 47, loss = 0.66260373\n","Iteration 48, loss = 0.66248404\n","Iteration 49, loss = 0.66237467\n","Iteration 50, loss = 0.66227512\n","Iteration 51, loss = 0.66218486\n","Iteration 52, loss = 0.66210339\n","Iteration 53, loss = 0.66203017\n","Iteration 54, loss = 0.66196468\n","Iteration 55, loss = 0.73136918\n","Iteration 56, loss = 0.66186760\n","Iteration 57, loss = 0.66183219\n","Iteration 58, loss = 0.66179999\n","Iteration 59, loss = 0.66177085\n","Iteration 60, loss = 0.66174459\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","The accuracy is:  0.7142857142857143\n"]}],"source":["#Multi Layer Perceptron\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","data = pd.read_excel(\"xyz.xlsx\")\n","print(data.head())\n","X = data[['protocol address','type of connection', 'number of packets', 'data transfer rate']]\n","y = data['purchased']\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\n","clf  = MLPClassifier(hidden_layer_sizes = (3,3),\n","                     random_state = 3,\n","                     verbose = True,\n","                     learning_rate_init = 0.003)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print(\"The accuracy is: \", accuracy_score(y_test,y_pred))"]},{"cell_type":"code","source":["\n","#bins are used to categorize data into intervals\n","#encoder is used to convert categorical value into numerical value\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","# Load dataset\n","df = pd.read_csv('house-prices.csv')\n","\n","# Data preprocessing\n","# Assuming 'Price' is the target column and we convert it to categories\n","bins = [0, 100000, 300000, 500000, np.inf]\n","labels = ['Low', 'Medium', 'High', 'Very High']\n","df['Price Category'] = pd.cut(df['Price'], bins=bins, labels=labels)\n","\n","# Encoding categorical target variable\n","label_encoder = LabelEncoder()\n","df['Price Category'] = label_encoder.fit_transform(df['Price Category'])\n","\n","# Splitting features and target\n","X = df.drop(columns=['Price', 'Price Category'])  # Drop target column and original price\n","y = df['Price Category']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Convert categorical features to numerical using one-hot encoding\n","X_train = pd.get_dummies(X_train, drop_first=True)\n","X_test = pd.get_dummies(X_test, drop_first=True)\n","\n","# Align columns in training and testing sets\n","# This ensures both sets have the same columns after one-hot encoding\n","X_train, X_test = X_train.align(X_test, join='outer', axis=1, fill_value=0)\n","\n","# Impute missing values after one-hot encoding\n","X_train.fillna(X_train.mean(), inplace=True)\n","X_test.fillna(X_test.mean(), inplace=True)\n","\n","# Standardize features\n","scaler = StandardScaler() # Initialize scaler here\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Define MLP model\n","def create_mlp(input_dim):\n","    model = Sequential([\n","        Dense(128, activation='relu', input_shape=(input_dim,)),\n","        Dropout(0.3),\n","        Dense(64, activation='relu'),\n","        Dropout(0.2),\n","        Dense(32, activation='relu'),\n","        Dense(4, activation='softmax')  # 4 output classes\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","    # Create and train the model\n","mlp_model = create_mlp(X_train.shape[1])\n","history = mlp_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16)\n","\n","# Fine-tuning with additional training\n","def fine_tune_model(model, X_train, y_train, X_test, y_test, epochs=20):\n","    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=16)\n","    return model\n","\n","# Fine-tune the model\n","mlp_model = fine_tune_model(mlp_model, X_train, y_train, X_test, y_test)\n","\n","# Evaluate the model\n","test_loss, test_acc = mlp_model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {test_acc:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GoYxOtT0fgVo","executionInfo":{"status":"ok","timestamp":1739170092732,"user_tz":-330,"elapsed":25326,"user":{"displayName":"Shreya Charlie","userId":"14569113716530734218"}},"outputId":"3095d30c-551a-4cd4-e7e3-685ab4afbdc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 84ms/step - accuracy: 0.2255 - loss: 1.4188 - val_accuracy: 0.8077 - val_loss: 1.2159\n","Epoch 2/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8011 - loss: 1.1556 - val_accuracy: 0.8846 - val_loss: 0.9864\n","Epoch 3/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9162 - loss: 0.9421 - val_accuracy: 0.8846 - val_loss: 0.7667\n","Epoch 4/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9182 - loss: 0.6980 - val_accuracy: 0.8846 - val_loss: 0.5893\n","Epoch 5/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9027 - loss: 0.4839 - val_accuracy: 0.8846 - val_loss: 0.4903\n","Epoch 6/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9210 - loss: 0.3576 - val_accuracy: 0.8846 - val_loss: 0.4557\n","Epoch 7/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8756 - loss: 0.3909 - val_accuracy: 0.8846 - val_loss: 0.4375\n","Epoch 8/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8999 - loss: 0.2872 - val_accuracy: 0.8846 - val_loss: 0.4213\n","Epoch 9/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9042 - loss: 0.2858 - val_accuracy: 0.8846 - val_loss: 0.3862\n","Epoch 10/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9352 - loss: 0.2002 - val_accuracy: 0.8846 - val_loss: 0.3634\n","Epoch 11/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2178 - val_accuracy: 0.8846 - val_loss: 0.3422\n","Epoch 12/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9217 - loss: 0.1843 - val_accuracy: 0.8846 - val_loss: 0.3237\n","Epoch 13/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8925 - loss: 0.1923 - val_accuracy: 0.8846 - val_loss: 0.2987\n","Epoch 14/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9340 - loss: 0.1526 - val_accuracy: 0.8846 - val_loss: 0.3102\n","Epoch 15/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9109 - loss: 0.1682 - val_accuracy: 0.8846 - val_loss: 0.3194\n","Epoch 16/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9577 - loss: 0.1147 - val_accuracy: 0.8846 - val_loss: 0.3450\n","Epoch 17/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9317 - loss: 0.1198 - val_accuracy: 0.8846 - val_loss: 0.3279\n","Epoch 18/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9016 - loss: 0.1624 - val_accuracy: 0.8846 - val_loss: 0.3196\n","Epoch 19/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9010 - loss: 0.1987 - val_accuracy: 0.8846 - val_loss: 0.3314\n","Epoch 20/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9393 - loss: 0.1296 - val_accuracy: 0.8846 - val_loss: 0.3605\n","Epoch 21/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9368 - loss: 0.1295 - val_accuracy: 0.8846 - val_loss: 0.3867\n","Epoch 22/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9103 - loss: 0.1631 - val_accuracy: 0.8846 - val_loss: 0.3583\n","Epoch 23/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9365 - loss: 0.1267 - val_accuracy: 0.8846 - val_loss: 0.3412\n","Epoch 24/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9402 - loss: 0.1297 - val_accuracy: 0.8846 - val_loss: 0.3569\n","Epoch 25/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9834 - loss: 0.1039 - val_accuracy: 0.8846 - val_loss: 0.3975\n","Epoch 26/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.9023 - loss: 0.1839 - val_accuracy: 0.8846 - val_loss: 0.4435\n","Epoch 27/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9403 - loss: 0.0928 - val_accuracy: 0.8846 - val_loss: 0.4596\n","Epoch 28/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9118 - loss: 0.1454 - val_accuracy: 0.8846 - val_loss: 0.4952\n","Epoch 29/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9599 - loss: 0.1136 - val_accuracy: 0.8846 - val_loss: 0.4532\n","Epoch 30/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9568 - loss: 0.0835 - val_accuracy: 0.8846 - val_loss: 0.4268\n","Epoch 31/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9461 - loss: 0.0794 - val_accuracy: 0.8846 - val_loss: 0.4801\n","Epoch 32/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9448 - loss: 0.1110 - val_accuracy: 0.8846 - val_loss: 0.5032\n","Epoch 33/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9455 - loss: 0.0917 - val_accuracy: 0.8846 - val_loss: 0.5235\n","Epoch 34/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9113 - loss: 0.1317 - val_accuracy: 0.8846 - val_loss: 0.5265\n","Epoch 35/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9621 - loss: 0.0887 - val_accuracy: 0.8846 - val_loss: 0.4994\n","Epoch 36/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9377 - loss: 0.0953 - val_accuracy: 0.8846 - val_loss: 0.5201\n","Epoch 37/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9595 - loss: 0.1006 - val_accuracy: 0.8846 - val_loss: 0.5475\n","Epoch 38/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9634 - loss: 0.1022 - val_accuracy: 0.8846 - val_loss: 0.5985\n","Epoch 39/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9760 - loss: 0.0834 - val_accuracy: 0.8846 - val_loss: 0.6257\n","Epoch 40/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9763 - loss: 0.0696 - val_accuracy: 0.8846 - val_loss: 0.6148\n","Epoch 41/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9865 - loss: 0.0602 - val_accuracy: 0.8846 - val_loss: 0.6348\n","Epoch 42/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9492 - loss: 0.0857 - val_accuracy: 0.8846 - val_loss: 0.6247\n","Epoch 43/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9756 - loss: 0.0683 - val_accuracy: 0.8846 - val_loss: 0.5942\n","Epoch 44/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9609 - loss: 0.0810 - val_accuracy: 0.8846 - val_loss: 0.6118\n","Epoch 45/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9947 - loss: 0.0600 - val_accuracy: 0.8846 - val_loss: 0.6584\n","Epoch 46/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9894 - loss: 0.0487 - val_accuracy: 0.8846 - val_loss: 0.6946\n","Epoch 47/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9838 - loss: 0.0540 - val_accuracy: 0.8846 - val_loss: 0.7682\n","Epoch 48/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9616 - loss: 0.0891 - val_accuracy: 0.8846 - val_loss: 0.7977\n","Epoch 49/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9673 - loss: 0.0514 - val_accuracy: 0.8846 - val_loss: 0.7299\n","Epoch 50/50\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9903 - loss: 0.0424 - val_accuracy: 0.8846 - val_loss: 0.7586\n","Epoch 1/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9328 - loss: 0.0948 - val_accuracy: 0.8846 - val_loss: 0.8047\n","Epoch 2/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9712 - loss: 0.0664 - val_accuracy: 0.8846 - val_loss: 0.8357\n","Epoch 3/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9848 - loss: 0.0505 - val_accuracy: 0.8846 - val_loss: 0.7721\n","Epoch 4/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9206 - loss: 0.1027 - val_accuracy: 0.8846 - val_loss: 0.7445\n","Epoch 5/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9848 - loss: 0.0525 - val_accuracy: 0.8846 - val_loss: 0.7519\n","Epoch 6/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9658 - loss: 0.0662 - val_accuracy: 0.8846 - val_loss: 0.8013\n","Epoch 7/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9975 - loss: 0.0255 - val_accuracy: 0.8846 - val_loss: 0.8186\n","Epoch 8/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9731 - loss: 0.0612 - val_accuracy: 0.8846 - val_loss: 0.7747\n","Epoch 9/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9531 - loss: 0.0651 - val_accuracy: 0.8846 - val_loss: 0.7725\n","Epoch 10/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9975 - loss: 0.0298 - val_accuracy: 0.8846 - val_loss: 0.7975\n","Epoch 11/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9962 - loss: 0.0342 - val_accuracy: 0.8846 - val_loss: 0.9707\n","Epoch 12/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9731 - loss: 0.0791 - val_accuracy: 0.8846 - val_loss: 1.0176\n","Epoch 13/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9685 - loss: 0.0471 - val_accuracy: 0.8846 - val_loss: 1.0256\n","Epoch 14/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9927 - loss: 0.0281 - val_accuracy: 0.8846 - val_loss: 1.0186\n","Epoch 15/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9901 - loss: 0.0345 - val_accuracy: 0.8846 - val_loss: 1.0163\n","Epoch 16/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9975 - loss: 0.0255 - val_accuracy: 0.8846 - val_loss: 0.9918\n","Epoch 17/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9927 - loss: 0.0342 - val_accuracy: 0.8846 - val_loss: 0.9092\n","Epoch 18/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9760 - loss: 0.0456 - val_accuracy: 0.8846 - val_loss: 0.9299\n","Epoch 19/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9862 - loss: 0.0257 - val_accuracy: 0.8846 - val_loss: 0.9900\n","Epoch 20/20\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0198 - val_accuracy: 0.8846 - val_loss: 1.0198\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8846 - loss: 1.0198\n","Test Accuracy: 0.88\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"oE-ck1i-iq3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QdXOAl0bi1vW"},"execution_count":null,"outputs":[]}]}